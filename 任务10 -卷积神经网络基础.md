1. 卷积运算的定义、动机（稀疏权重、参数共享、等变表示）。一维卷积运算和二维卷积运算。
    ![img](https://github.com/lbj000/nlp/blob/master/卷积运算.gif)  
    参数共享机制
　　•	在卷积层中每个神经元连接数据窗的权重是固定的，每个神经元只关注一个特性。  
      神经元就是图像处理中的滤波器，比如边缘检测专用的Sobel滤波器，  
      即卷积层的每个滤波器都会有自己所关注一个图像特征，比如垂直边缘，水平边缘，  
      颜色，纹理等等，这些所有神经元加起来就好比就是整张图像的特征提取器集合。  
　　•	需要估算的权重个数减少: AlexNet 1亿 => 3.5w  
　　•	一组固定的权重和不同窗口内数据做内积: 卷积  
    ![img](https://github.com/lbj000/nlp/blob/master/参数共享.png)  
    
    图像处理中的二维卷积

    二维卷积就是一维卷积的扩展，原理差不多。核心还是（反转），移动，乘积，求和。  
    这里二维的反转就是将卷积核沿反对角线翻转，比如：
    
    ![img](https://github.com/lbj000/nlp/blob/master/二维卷积1.png)  
    之后，卷积核在二维平面上平移，并且卷积核的每个元素与被卷积图像对应位置相乘，  
    再求和。通过卷积核的不断移动，我们就有了一个新的图像，这个图像完全由卷积核在各个位置时的乘积求和的结果组成。
    举一个最简单的均值滤波的例子：
    ![img](https://github.com/lbj000/nlp/blob/master/二维卷积2.png)  
    当卷积核运动到图像右下角处（卷积中心和图像对应图像第 4 行第 4 列）时，它和图像卷积的结果如下图所示：
    ![img](https://github.com/lbj000/nlp/blob/master/二维卷积3.png)  
    可以看出，二维卷积在图像中的效果就是：对图像的每个像素的邻域（邻域大小就是核的大小）  
    加权求和得到该像素点的输出值。滤波器核在这里是作为一个“权重表”来使用的。
2. 反卷积(tf.nn.conv2d_transpose)

    ![img](https://github.com/lbj000/nlp/blob/master/反卷积1.gif)  
    
    ![img](https://github.com/lbj000/nlp/blob/master/反卷积2.gif)  
    
3. 池化运算的定义、种类（最大池化、平均池化等）、动机。
    池化层夹在连续的卷积层中间， 用于压缩数据和参数的量，减小过拟合。
    简而言之，如果输入是图像的话，那么池化层的最主要作用就是压缩图像。
    这里再展开叙述池化层的具体作用。
    1. 特征不变性，也就是我们在图像处理中经常提到的特征的尺度不变性，  
    池化操作就是图像的resize，平时一张狗的图像被缩小了一倍我们还能认出这是一张狗的照片，  
    这说明这张图像中仍保留着狗最重要的特征，我们一看就能判断图像中画的是一只狗，  
    图像压缩时去掉的信息只是一些无关紧要的信息，而留下的信息则是具有尺度不变性的特征，是最能表达图像的特征。

    2. 特征降维，我们知道一幅图像含有的信息是很大的，特征也很多，  
    但是有些信息对于我们做图像任务时没有太多用途或者有重复，我们可以把这类冗余信息去除，  
    把最重要的特征抽取出来，这也是池化操作的一大作用。

    3. 在一定程度上防止过拟合，更方便优化。  
    ![img](https://github.com/lbj000/nlp/blob/master/池化1.jpg)  
    
    池化层用的方法有Max pooling 和 average pooling，而实际用的较多的是Max pooling。
    这里就说一下Max pooling，其实思想非常简单。
    
    ![img](https://github.com/lbj000/nlp/blob/master/池化2.jpg)  
    
    对于每个2*2的窗口选出最大的数作为输出矩阵的相应元素的值，比如输入矩阵第一个2*2窗口中最大的数是6，  
    那么输出矩阵的第一个元素就是6，如此类推。
4. Text-CNN的原理。
    下面讲解下CNN的分类过程。
    我们知道CNN通常用来做图像处理的，而在最开始图像输入的时候会有1到3个通道，1是黑白图像，3是彩色图像。
    但与图像的通道不同的是，大多数NLP任务的输入是作为一个矩阵表示的句子或文档。  
    矩阵的每一行对应一个token，通常是一个单词，但它可以是一个字符。也就是说，  
    每一行都是表示一个单词的向量。通常，这些向量是单词嵌入（低维度表示），比如word2vec或GloVe，  
    但它们也可以是一个one-hot的向量，将单词编入一个词汇表，但可想这个维度十分巨大，  
    比如这里5000个词的词汇表每个单词的ont-hot就有5000维，空间要炸了：）。  
    对于一个使用100维嵌入的10个单词的句子，我们将会有一个10*100的矩阵作为输入，  
    这就相当于CNN做图像处理时的图像输入层，此处的100就是CNN中的通道数。
    处理图像时，filter在图像的局部补丁上滑动，但是在NLP中，我们使用的filter要滑过矩阵每个单词的全行。  
    因此，我们的filter的“宽度”通常与输入矩阵的宽度相同。高度或区域大小可能有所不同，  
    但每次滑动窗口超过2-5个单词是正常的。把所有这些放在一起，一个用于NLP的卷积神经网络可能是这样的：
    
    ![img](https://github.com/lbj000/nlp/blob/master/Text-CNN.png)  
    
5. 利用Text-CNN模型来进行文本分类。

    ![img](https://github.com/lbj000/nlp/blob/master/分类结果.png)  

    可以从图中看出分类结果比较理想，F1值达到95.3%，并且召回率和准确率都达到了95.4%，  
    整体分类效果较好。稍逊于原论文作者的96%，下次尝试完整复现论文。其中可知教育分类较一般，  
    家具和时政分类结果较一般的好。如果需要提高分类准确率，可以通过调节dropout来实现，  
    或者在词嵌入层提前用Word2vec训练好词向量，不用cnn反向传播更新词向量，也许效果会更好，  
    当然也有可能教育类、家具类、时政类文章的长度比较短，而这里基于所有文章同样长度来做训练，  
    导致了预测不佳的情况。所以本分类实现对文章的长度是有要求的。

