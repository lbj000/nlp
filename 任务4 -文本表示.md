1.TF-IDF原理。  
  TF-IDF是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。
  字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。
  
  有很多不同的数学公式可以用来计算TF-IDF。这边的例子以上述的数学公式来计算。词频 (TF) 是一词语
  出现的次数除以该文件的总词语数。假如一篇文件的总词语数是100个，而词语“母牛”出现了3次，那么“母牛”
  一词在该文件中的词频就是3/100=0.03。一个计算文件频率 (IDF) 的方法是文件集里包含的文件总数除以测
  定有多少份文件出现过“母牛”一词。所以，如果“母牛”一词在1,000份文件出现过，而文件总数是10,000,000
  份的话，其逆向文件频率就是 lg(10,000,000 / 1,000)=4。最后的TF-IDF的分数为0.03 * 4=0.12。  
  
2. 文本矩阵化，使用词袋模型，以TF-IDF特征值为权重。（可以使用Python中TfidfTransformer库）  
![img](https://github.com/lbj000/nlp/blob/master/TF-IDF向量化文档.png)  

![img](https://github.com/lbj000/nlp/blob/master/输出的各个文本各个词的TF-IDF值如下.png)  

3. 互信息的原理。  
  互信息（mutual information）是用来评价一个事件的出现对于另一个事件的出现所贡献的信息量
  使用互信息理论进行特征抽取是基于如下假设:在某个特定类别出现频率高,但在其他类别出现频率比
  较低的词条与该类的互信息比较大。通常用互信息作为特征词和类别之问的测度，如果特征词属于该
  类的话，它们的互信息量最大。由于该方法不需要对特征词和类别之问关系的性质作任何假设，因此
  非常适合于文本分类的特征和类别的配准工作。特征项和类别的互信息体现了特征项与类别的相关程
  度,是一种广泛用于建立词关联统计模型的标准。互信息与期望交叉熵的不同在于没有考虑特征出现
  的频率,这样导致互信息评估函数不选择高频的有用词而有可能选择稀有词作为文本的最佳特征。因
  为对于每一主题来讲，特征t的互信息越大，说明它与该主题的共现概率越大，因此，以互信息作为
  提取特征的评价时应选互信息最大的若干个特征。互信息计算的时间复杂度类似于信息增益,互信息
  的平均值就是信息增益。互信息的不足之处在于得分非常受词条边缘概率的影响。  
  
4. 使用第二步生成的特征矩阵，利用互信息进行特征筛选。  
 （还不会）
