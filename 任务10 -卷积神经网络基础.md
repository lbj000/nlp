1. 卷积运算的定义、动机（稀疏权重、参数共享、等变表示）。一维卷积运算和二维卷积运算。
    ![img](https://github.com/lbj000/nlp/blob/master/卷积运算.gif)  
    参数共享机制
　　•	在卷积层中每个神经元连接数据窗的权重是固定的，每个神经元只关注一个特性。  
      神经元就是图像处理中的滤波器，比如边缘检测专用的Sobel滤波器，  
      即卷积层的每个滤波器都会有自己所关注一个图像特征，比如垂直边缘，水平边缘，  
      颜色，纹理等等，这些所有神经元加起来就好比就是整张图像的特征提取器集合。  
　　•	需要估算的权重个数减少: AlexNet 1亿 => 3.5w  
　　•	一组固定的权重和不同窗口内数据做内积: 卷积  
    ![img](https://github.com/lbj000/nlp/blob/master/参数共享.png)  
    
    图像处理中的二维卷积

    二维卷积就是一维卷积的扩展，原理差不多。核心还是（反转），移动，乘积，求和。  
    这里二维的反转就是将卷积核沿反对角线翻转，比如：
    
    ![img](https://github.com/lbj000/nlp/blob/master/二维卷积1.png)  
    之后，卷积核在二维平面上平移，并且卷积核的每个元素与被卷积图像对应位置相乘，  
    再求和。通过卷积核的不断移动，我们就有了一个新的图像，这个图像完全由卷积核在各个位置时的乘积求和的结果组成。
    举一个最简单的均值滤波的例子：
    ![img](https://github.com/lbj000/nlp/blob/master/二维卷积2.png)  
    当卷积核运动到图像右下角处（卷积中心和图像对应图像第 4 行第 4 列）时，它和图像卷积的结果如下图所示：
    ![img](https://github.com/lbj000/nlp/blob/master/二维卷积3.png)  
    可以看出，二维卷积在图像中的效果就是：对图像的每个像素的邻域（邻域大小就是核的大小）  
    加权求和得到该像素点的输出值。滤波器核在这里是作为一个“权重表”来使用的。
2. 反卷积(tf.nn.conv2d_transpose)

    ![img](https://github.com/lbj000/nlp/blob/master/反卷积1.gif)  
    
    ![img](https://github.com/lbj000/nlp/blob/master/反卷积2.gif)  
    
3. 池化运算的定义、种类（最大池化、平均池化等）、动机。
    池化层夹在连续的卷积层中间， 用于压缩数据和参数的量，减小过拟合。
    简而言之，如果输入是图像的话，那么池化层的最主要作用就是压缩图像。
    这里再展开叙述池化层的具体作用。
    1. 特征不变性，也就是我们在图像处理中经常提到的特征的尺度不变性，  
    池化操作就是图像的resize，平时一张狗的图像被缩小了一倍我们还能认出这是一张狗的照片，  
    这说明这张图像中仍保留着狗最重要的特征，我们一看就能判断图像中画的是一只狗，  
    图像压缩时去掉的信息只是一些无关紧要的信息，而留下的信息则是具有尺度不变性的特征，是最能表达图像的特征。

    2. 特征降维，我们知道一幅图像含有的信息是很大的，特征也很多，  
    但是有些信息对于我们做图像任务时没有太多用途或者有重复，我们可以把这类冗余信息去除，  
    把最重要的特征抽取出来，这也是池化操作的一大作用。

    3. 在一定程度上防止过拟合，更方便优化。  
    ![img](https://github.com/lbj000/nlp/blob/master/池化1.gif)  
    
    池化层用的方法有Max pooling 和 average pooling，而实际用的较多的是Max pooling。
    这里就说一下Max pooling，其实思想非常简单。
    
    ![img](https://github.com/lbj000/nlp/blob/master/池化2.gif)  
    
    对于每个2*2的窗口选出最大的数作为输出矩阵的相应元素的值，比如输入矩阵第一个2*2窗口中最大的数是6，  
    那么输出矩阵的第一个元素就是6，如此类推。
4. Text-CNN的原理。
5. 利用Text-CNN模型来进行文本分类。
