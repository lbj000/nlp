1. 文本表示：从one-hot到word2vec。  
1.1 词袋模型：离散、高维、稀疏。  
1.2 分布式表示：连续、低维、稠密。word2vec词向量原理并实践，用来表示文本。  
独热编码

独热编码即 One-Hot 编码，又称一位有效编码，其方法是使用N位状态寄存器来对N个状态进行编码，  
每个状态都有它独立的寄存器位，并且在任意时候，其中只有一位有效。举个例子，假设我们有四个样本（行），  
每个样本有三个特征（列），如图：  
    ![img](https://github.com/lbj000/nlp/blob/master/onehot1.png)  
我们的feature_1有两种可能的取值，比如是男/女，这里男用1表示，女用2表示。feature_2 和feature_3各有4种取值（状态）。  
one-hot编码就是保证每个样本中的单个特征只有1位处于状态1，其他的都是0。上述状态用one-hot编码如下图所示：  
    ![img](https://github.com/lbj000/nlp/blob/master/onehot2.png)  
考虑一下的三个特征：  
["male", "female"]  
["from Europe", "from US", "from Asia"]  
["uses Firefox", "uses Chrome", "uses Safari", "uses Internet Explorer"]  

将它换成独热编码后，应该是：  
feature1=[01,10]  
feature2=[001,010,100]  
feature3=[0001,0010,0100,1000]  
优缺点分析  
优点：一是解决了分类器不好处理离散数据的问题，二是在一定程度上也起到了扩充特征的作用。  
缺点：在文本特征表示上有些缺点就非常突出了。首先，它是一个词袋模型，不考虑词与词之间的顺序（文本中词的顺序信息也是很重要的）；  
其次，它假设词与词相互独立（在大多数情况下，词与词是相互影响的）；最后，它得到的特征是离散稀疏的。  
为什么得到的特征是离散稀疏的？  
上面举例比较简单，但现实情况可能不太一样。比如如果将世界所有城市名称作为语料库的话，那这个向量会过于稀疏，并且会造成维度灾难。  
杭州 [0,0,0,0,0,0,0,1,0,……，0,0,0,0,0,0,0]  
上海 [0,0,0,0,1,0,0,0,0,……，0,0,0,0,0,0,0]  
宁波 [0,0,0,1,0,0,0,0,0,……，0,0,0,0,0,0,0]  
北京 [0,0,0,0,0,0,0,0,0,……，1,0,0,0,0,0,0]  
在语料库中，杭州、上海、宁波、北京各对应一个向量，向量中只有一个值为1，其余都为0。  
能不能把词向量的维度变小呢？  
Dristributed representation可以解决One hot representation的问题，它的思路是通过训练，将每个词都映射到一个较短的词向量上来。  
所有的这些词向量就构成了向量空间，进而可以用普通的统计学的方法来研究词与词之间的关系。这个较短的词向量维度是多大呢？  
这个一般需要我们在训练时自己来指定。  
比如下图我们将词汇表里的词用"Royalty","Masculinity", "Femininity"和"Age"4个维度来表示，  
King这个词对应的词向量可能是(0.99,0.99,0.05,0.7)(0.99,0.99,0.05,0.7)。当然在实际情况中，我们并不能对词向量的每个维度做一个很好的解释。  
    ![img](https://github.com/lbj000/nlp/blob/master/onehot3.png)  
我们将king这个词从一个可能非常稀疏的向量坐在的空间，映射到现在这个四维向量所在的空间，必须满足以下性质：  

（1）这个映射是单设（不懂的概念自行搜索）；   
（2）映射之后的向量不会丢失之前的那种向量所含的信息。  
这个过程称为word embedding（词嵌入），即将高维词向量嵌入到一个低维空间。顺便找了个图  
    ![img](https://github.com/lbj000/nlp/blob/master/onehot4.png)  

word2vec

word2vec模型其实就是简单化的神经网络。  
    ![img](https://github.com/lbj000/nlp/blob/master/word2vec1.png)  
输入是One-Hot Vector，Hidden Layer没有激活函数，也就是线性的单元。  
Output Layer维度跟Input Layer的维度一样，用的是Softmax回归。当这个模型训练好以后，  
我们并不会用这个训练好的模型处理新的任务，我们真正需要的是这个模型通过训练数据所学得的参数，例如隐层的权重矩阵。  

这个模型是如何定义数据的输入和输出呢？一般分为CBOW(Continuous Bag-of-Words 与Skip-Gram两种模型。  
CBOW模型的训练输入是某一个特征词的上下文相关的词对应的词向量，而输出就是这特定的一个词的词向量。  
Skip-Gram模型和CBOW的思路是反着来的，即输入是特定的一个词的词向量，而输出是特定词对应的上下文词向量。  
CBOW对小型数据库比较合适，而Skip-Gram在大型语料中表现更好。
    ![img](https://github.com/lbj000/nlp/blob/master/word2vec2.png)  
    ![img](https://github.com/lbj000/nlp/blob/master/word2vec3.png)  
    
    
作者：缺省之名
链接：https://www.jianshu.com/p/471d9bfbd72f
来源：简书
简书著作权归作者所有，任何形式的转载都请联系作者获得授权并注明出处。

